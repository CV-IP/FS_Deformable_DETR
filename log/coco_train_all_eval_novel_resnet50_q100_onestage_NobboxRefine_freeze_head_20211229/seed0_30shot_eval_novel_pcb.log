| distributed init (rank 0): env://
| distributed init (rank 7): env://
| distributed init (rank 1): env://
| distributed init (rank 2): env://
| distributed init (rank 4): env://
| distributed init (rank 3): env://
| distributed init (rank 5): env://
| distributed init (rank 6): env://
n128-097-212:519189:519189 [0] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519189:519189 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519189:519189 [0] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519189:519189 [0] NCCL INFO Using network IB
NCCL version 2.7.8+cuda10.2
n128-097-212:519202:519202 [7] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519193:519193 [2] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519197:519197 [4] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519199:519199 [5] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519201:519201 [6] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519191:519191 [1] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519195:519195 [3] NCCL INFO Bootstrap : Using [0]eth0:10.128.97.212<0> [1]eth2:10.128.98.14<0>
n128-097-212:519193:519193 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519202:519202 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519199:519199 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519201:519201 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519195:519195 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519197:519197 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519191:519191 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
n128-097-212:519191:519191 [1] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519197:519197 [4] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519191:519191 [1] NCCL INFO Using network IB
n128-097-212:519197:519197 [4] NCCL INFO Using network IB
n128-097-212:519202:519202 [7] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519199:519199 [5] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519202:519202 [7] NCCL INFO Using network IB
n128-097-212:519195:519195 [3] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519199:519199 [5] NCCL INFO Using network IB
n128-097-212:519201:519201 [6] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519195:519195 [3] NCCL INFO Using network IB
n128-097-212:519193:519193 [2] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [1]mlx5_0:1/RoCE ; OOB eth0:10.128.97.212<0>
n128-097-212:519201:519201 [6] NCCL INFO Using network IB
n128-097-212:519193:519193 [2] NCCL INFO Using network IB
n128-097-212:519195:519772 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519197:519766 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519199:519769 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519195:519772 [3] NCCL INFO Trees [0] 4/-1/-1->3->2|2->3->4/-1/-1 [1] 1/-1/-1->3->2|2->3->1/-1/-1 [2] -1/-1/-1->3->1|1->3->-1/-1/-1 [3] 2/-1/-1->3->1|1->3->2/-1/-1 [4] 2/-1/-1->3->4|4->3->2/-1/-1 [5] 1/-1/-1->3->0|0->3->1/-1/-1 [6] 4/-1/-1->3->2|2->3->4/-1/-1 [7] 1/-1/-1->3->2|2->3->1/-1/-1 [8] -1/-1/-1->3->1|1->3->-1/-1/-1 [9] 2/-1/-1->3->1|1->3->2/-1/-1 [10] 2/-1/-1->3->4|4->3->2/-1/-1 [11] 1/-1/-1->3->0|0->3->1/-1/-1
n128-097-212:519201:519771 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519199:519769 [5] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 7/-1/-1->5->4|4->5->7/-1/-1 [2] 7/-1/-1->5->2|2->5->7/-1/-1 [3] 4/-1/-1->5->7|7->5->4/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 2/-1/-1->5->6|6->5->2/-1/-1 [6] 6/-1/-1->5->4|4->5->6/-1/-1 [7] 7/-1/-1->5->4|4->5->7/-1/-1 [8] 7/-1/-1->5->2|2->5->7/-1/-1 [9] 4/-1/-1->5->7|7->5->4/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 2/-1/-1->5->6|6->5->2/-1/-1
n128-097-212:519189:519751 [0] NCCL INFO Channel 00/12 :    0   1   2   3   4   5   6   7
n128-097-212:519202:519768 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519197:519766 [4] NCCL INFO Trees [0] 5/-1/-1->4->3|3->4->5/-1/-1 [1] 5/-1/-1->4->6|6->4->5/-1/-1 [2] 6/-1/-1->4->7|7->4->6/-1/-1 [3] 6/-1/-1->4->5|5->4->6/-1/-1 [4] 3/-1/-1->4->5|5->4->3/-1/-1 [5] 7/-1/-1->4->-1|-1->4->7/-1/-1 [6] 5/-1/-1->4->3|3->4->5/-1/-1 [7] 5/-1/-1->4->6|6->4->5/-1/-1 [8] 6/-1/-1->4->7|7->4->6/-1/-1 [9] 6/-1/-1->4->5|5->4->6/-1/-1 [10] 3/-1/-1->4->5|5->4->3/-1/-1 [11] 7/-1/-1->4->-1|-1->4->7/-1/-1
n128-097-212:519191:519767 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519193:519770 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519195:519772 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
n128-097-212:519199:519769 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
n128-097-212:519189:519751 [0] NCCL INFO Channel 01/12 :    0   2   3   1   6   4   5   7
n128-097-212:519201:519771 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 4/-1/-1->6->1|1->6->4/-1/-1 [2] 1/-1/-1->6->4|4->6->1/-1/-1 [3] 1/-1/-1->6->4|4->6->1/-1/-1 [4] -1/-1/-1->6->1|1->6->-1/-1/-1 [5] 5/-1/-1->6->7|7->6->5/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 4/-1/-1->6->1|1->6->4/-1/-1 [8] 1/-1/-1->6->4|4->6->1/-1/-1 [9] 1/-1/-1->6->4|4->6->1/-1/-1 [10] -1/-1/-1->6->1|1->6->-1/-1/-1 [11] 5/-1/-1->6->7|7->6->5/-1/-1
n128-097-212:519197:519766 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
n128-097-212:519202:519768 [7] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->5|5->7->-1/-1/-1 [2] 4/-1/-1->7->5|5->7->4/-1/-1 [3] 5/-1/-1->7->0|0->7->5/-1/-1 [4] 5/-1/-1->7->0|0->7->5/-1/-1 [5] 6/-1/-1->7->4|4->7->6/-1/-1 [6] -1/-1/-1->7->6|6->7->-1/-1/-1 [7] -1/-1/-1->7->5|5->7->-1/-1/-1 [8] 4/-1/-1->7->5|5->7->4/-1/-1 [9] 5/-1/-1->7->0|0->7->5/-1/-1 [10] 5/-1/-1->7->0|0->7->5/-1/-1 [11] 6/-1/-1->7->4|4->7->6/-1/-1
n128-097-212:519191:519767 [1] NCCL INFO Trees [0] 2/-1/-1->1->0|0->1->2/-1/-1 [1] 6/-1/-1->1->3|3->1->6/-1/-1 [2] 3/-1/-1->1->6|6->1->3/-1/-1 [3] 3/-1/-1->1->6|6->1->3/-1/-1 [4] 6/-1/-1->1->2|2->1->6/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 2/-1/-1->1->0|0->1->2/-1/-1 [7] 6/-1/-1->1->3|3->1->6/-1/-1 [8] 3/-1/-1->1->6|6->1->3/-1/-1 [9] 3/-1/-1->1->6|6->1->3/-1/-1 [10] 6/-1/-1->1->2|2->1->6/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1
n128-097-212:519189:519751 [0] NCCL INFO Channel 02/12 :    0   2   5   7   4   6   1   3
n128-097-212:519201:519771 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
n128-097-212:519193:519770 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->0|0->2->3/-1/-1 [2] 5/-1/-1->2->0|0->2->5/-1/-1 [3] -1/-1/-1->2->3|3->2->-1/-1/-1 [4] 1/-1/-1->2->3|3->2->1/-1/-1 [5] 0/-1/-1->2->5|5->2->0/-1/-1 [6] 3/-1/-1->2->1|1->2->3/-1/-1 [7] 3/-1/-1->2->0|0->2->3/-1/-1 [8] 5/-1/-1->2->0|0->2->5/-1/-1 [9] -1/-1/-1->2->3|3->2->-1/-1/-1 [10] 1/-1/-1->2->3|3->2->1/-1/-1 [11] 0/-1/-1->2->5|5->2->0/-1/-1
n128-097-212:519202:519768 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
n128-097-212:519189:519751 [0] NCCL INFO Channel 03/12 :    0   7   5   4   6   1   3   2
n128-097-212:519191:519767 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
n128-097-212:519193:519770 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
n128-097-212:519189:519751 [0] NCCL INFO Channel 04/12 :    0   7   6   5   4   3   2   1
n128-097-212:519189:519751 [0] NCCL INFO Channel 05/12 :    0   3   1   6   4   7   5   2
n128-097-212:519189:519751 [0] NCCL INFO Channel 06/12 :    0   1   2   3   4   5   6   7
n128-097-212:519189:519751 [0] NCCL INFO Channel 07/12 :    0   2   3   1   6   4   5   7
n128-097-212:519189:519751 [0] NCCL INFO Channel 08/12 :    0   2   5   7   4   6   1   3
n128-097-212:519189:519751 [0] NCCL INFO Channel 09/12 :    0   7   5   4   6   1   3   2
n128-097-212:519189:519751 [0] NCCL INFO Channel 10/12 :    0   7   6   5   4   3   2   1
n128-097-212:519189:519751 [0] NCCL INFO Channel 11/12 :    0   3   1   6   4   7   5   2
n128-097-212:519189:519751 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64
n128-097-212:519189:519751 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 2/-1/-1->0->-1|-1->0->2/-1/-1 [2] 2/-1/-1->0->-1|-1->0->2/-1/-1 [3] 7/-1/-1->0->-1|-1->0->7/-1/-1 [4] 7/-1/-1->0->-1|-1->0->7/-1/-1 [5] 3/-1/-1->0->2|2->0->3/-1/-1 [6] 1/-1/-1->0->-1|-1->0->1/-1/-1 [7] 2/-1/-1->0->-1|-1->0->2/-1/-1 [8] 2/-1/-1->0->-1|-1->0->2/-1/-1 [9] 7/-1/-1->0->-1|-1->0->7/-1/-1 [10] 7/-1/-1->0->-1|-1->0->7/-1/-1 [11] 3/-1/-1->0->2|2->0->3/-1/-1
n128-097-212:519189:519751 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
n128-097-212:519197:519766 [4] NCCL INFO Channel 00 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 00 : 5[89000] -> 6[b1000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 00 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 00 : 6[b1000] -> 7[b2000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 00 : 3[3e000] -> 4[88000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 00 : 1[1b000] -> 2[3d000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 00 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 00 : 7[b2000] -> 6[b1000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 00 : 4[88000] -> 3[3e000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 00 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 00 : 6[b1000] -> 5[89000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 00 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 00 : 1[1b000] -> 0[1a000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 00 : 2[3d000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 01 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 01 : 0[1a000] -> 2[3d000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 01 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 01 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 01 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 01 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 01 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 01 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 01 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 01 : 4[88000] -> 6[b1000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 01 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 01 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 01 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 01 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 01 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 02 : 7[b2000] -> 4[88000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 02 : 0[1a000] -> 2[3d000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 02 : 4[88000] -> 6[b1000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 02 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 02 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 02 : 3[3e000] -> 0[1a000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 02 : 2[3d000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 02 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 02 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 02 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 02 : 4[88000] -> 7[b2000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 02 : 5[89000] -> 2[3d000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 02 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 02 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 02 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 03 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 03 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 03 : 0[1a000] -> 7[b2000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 03 : 4[88000] -> 6[b1000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 03 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 03 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 03 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 03 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 03 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 03 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 03 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 03 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 03 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 03 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 03 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 04 : 0[1a000] -> 7[b2000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 04 : 2[3d000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 04 : 7[b2000] -> 6[b1000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 04 : 4[88000] -> 3[3e000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 04 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 04 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 04 : 6[b1000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 04 : 1[1b000] -> 0[1a000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 04 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 04 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 04 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 04 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 04 : 3[3e000] -> 4[88000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 04 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 04 : 1[1b000] -> 2[3d000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 04 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 05 : 0[1a000] -> 3[3e000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 04 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 05 : 4[88000] -> 7[b2000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 05 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 05 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 05 : 5[89000] -> 2[3d000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 05 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 05 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 05 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 05 : 0[1a000] -> 2[3d000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 05 : 2[3d000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 05 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 05 : 3[3e000] -> 0[1a000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 05 : 5[89000] -> 6[b1000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 05 : 7[b2000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 05 : 6[b1000] -> 7[b2000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 06 : 1[1b000] -> 2[3d000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 06 : 0[1a000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 05 : 7[b2000] -> 6[b1000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 06 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 06 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 06 : 3[3e000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 05 : 6[b1000] -> 5[89000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 06 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 06 : 1[1b000] -> 0[1a000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 06 : 2[3d000] -> 1[1b000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 06 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 06 : 5[89000] -> 6[b1000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 06 : 6[b1000] -> 7[b2000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 06 : 7[b2000] -> 6[b1000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 06 : 4[88000] -> 3[3e000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 07 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 06 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 07 : 0[1a000] -> 2[3d000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 06 : 6[b1000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 07 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 07 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 07 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 07 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 07 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 07 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 07 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 07 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 07 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 07 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 07 : 4[88000] -> 6[b1000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 08 : 0[1a000] -> 2[3d000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 07 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 07 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 08 : 2[3d000] -> 5[89000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 08 : 7[b2000] -> 4[88000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 08 : 3[3e000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 08 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 08 : 4[88000] -> 6[b1000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 08 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 08 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 08 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 08 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 08 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 08 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 08 : 4[88000] -> 7[b2000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 08 : 5[89000] -> 2[3d000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 08 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 09 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 09 : 0[1a000] -> 7[b2000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 09 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 09 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 09 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 09 : 4[88000] -> 6[b1000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 09 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 09 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 09 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 09 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 09 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 09 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 09 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 09 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 09 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 10 : 2[3d000] -> 1[1b000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 10 : 0[1a000] -> 7[b2000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 10 : 3[3e000] -> 2[3d000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 10 : 7[b2000] -> 6[b1000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 10 : 1[1b000] -> 0[1a000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 10 : 4[88000] -> 3[3e000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 10 : 5[89000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 10 : 6[b1000] -> 5[89000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 10 : 2[3d000] -> 3[3e000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 10 : 3[3e000] -> 4[88000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 10 : 6[b1000] -> 1[1b000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 10 : 7[b2000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 10 : 1[1b000] -> 2[3d000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 10 : 4[88000] -> 5[89000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 10 : 5[89000] -> 7[b2000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 10 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 10 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 11 : 0[1a000] -> 3[3e000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 11 : 2[3d000] -> 0[1a000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 11 : 3[3e000] -> 1[1b000] via P2P/IPC
n128-097-212:519197:519766 [4] NCCL INFO Channel 11 : 4[88000] -> 7[b2000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 11 : 6[b1000] -> 4[88000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 11 : 1[1b000] -> 6[b1000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 11 : 5[89000] -> 2[3d000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 11 : 7[b2000] -> 5[89000] via P2P/IPC
n128-097-212:519189:519751 [0] NCCL INFO Channel 11 : 0[1a000] -> 2[3d000] via P2P/IPC
n128-097-212:519195:519772 [3] NCCL INFO Channel 11 : 3[3e000] -> 0[1a000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO Channel 11 : 1[1b000] -> 3[3e000] via P2P/IPC
n128-097-212:519193:519770 [2] NCCL INFO Channel 11 : 2[3d000] -> 5[89000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO Channel 11 : 7[b2000] -> 4[88000] via P2P/IPC
n128-097-212:519199:519769 [5] NCCL INFO Channel 11 : 5[89000] -> 6[b1000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 11 : 6[b1000] -> 7[b2000] via P2P/IPC
n128-097-212:519191:519767 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519189:519751 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519191:519767 [1] NCCL INFO comm 0x7f54e4001060 rank 1 nranks 8 cudaDev 1 busId 1b000 - Init COMPLETE
n128-097-212:519189:519751 [0] NCCL INFO comm 0x7fb0c8001060 rank 0 nranks 8 cudaDev 0 busId 1a000 - Init COMPLETE
n128-097-212:519189:519189 [0] NCCL INFO Launch mode Parallel
n128-097-212:519195:519772 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519195:519772 [3] NCCL INFO comm 0x7fca8c001060 rank 3 nranks 8 cudaDev 3 busId 3e000 - Init COMPLETE
n128-097-212:519197:519766 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519197:519766 [4] NCCL INFO comm 0x7f9d70001060 rank 4 nranks 8 cudaDev 4 busId 88000 - Init COMPLETE
n128-097-212:519193:519770 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519193:519770 [2] NCCL INFO comm 0x7fd0f0001060 rank 2 nranks 8 cudaDev 2 busId 3d000 - Init COMPLETE
n128-097-212:519202:519768 [7] NCCL INFO Channel 11 : 7[b2000] -> 6[b1000] via P2P/IPC
n128-097-212:519201:519771 [6] NCCL INFO Channel 11 : 6[b1000] -> 5[89000] via P2P/IPC
n128-097-212:519202:519768 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519202:519768 [7] NCCL INFO comm 0x7fab18001060 rank 7 nranks 8 cudaDev 7 busId b2000 - Init COMPLETE
n128-097-212:519199:519769 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519199:519769 [5] NCCL INFO comm 0x7f09a0001060 rank 5 nranks 8 cudaDev 5 busId 89000 - Init COMPLETE
n128-097-212:519201:519771 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer
n128-097-212:519201:519771 [6] NCCL INFO comm 0x7f81f8001060 rank 6 nranks 8 cudaDev 6 busId b1000 - Init COMPLETE
git:
  sha: 96645531f1c891d56fbd3dcc5e7915cc509a546c, status: clean, branch: fs_deformable_detr

number of params: 39742038
data/coco/cocosplit_self/seed0/full_box_30shot_trainval.json
id map
{1: 60, 2: 61, 3: 62, 4: 63, 5: 64, 6: 65, 7: 66, 9: 67, 16: 68, 17: 69, 18: 70, 19: 71, 20: 72, 21: 73, 44: 74, 62: 75, 63: 76, 64: 77, 67: 78, 72: 79, 8: 0, 10: 1, 11: 2, 13: 3, 14: 4, 15: 5, 22: 6, 23: 7, 24: 8, 25: 9, 27: 10, 28: 11, 31: 12, 32: 13, 33: 14, 34: 15, 35: 16, 36: 17, 37: 18, 38: 19, 39: 20, 40: 21, 41: 22, 42: 23, 43: 24, 46: 25, 47: 26, 48: 27, 49: 28, 50: 29, 51: 30, 52: 31, 53: 32, 54: 33, 55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 61: 40, 65: 41, 70: 42, 73: 43, 74: 44, 75: 45, 76: 46, 77: 47, 78: 48, 79: 49, 80: 50, 81: 51, 82: 52, 84: 53, 85: 54, 86: 55, 87: 56, 88: 57, 89: 58, 90: 59}
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
dataset_name : coco_all_seed_0_30_shot, image nums : 1187, anno nums : 2394
data/coco/cocosplit_self/datasplit/5k.json
id map
{1: 60, 2: 61, 3: 62, 4: 63, 5: 64, 6: 65, 7: 66, 9: 67, 16: 68, 17: 69, 18: 70, 19: 71, 20: 72, 21: 73, 44: 74, 62: 75, 63: 76, 64: 77, 67: 78, 72: 79, 8: 0, 10: 1, 11: 2, 13: 3, 14: 4, 15: 5, 22: 6, 23: 7, 24: 8, 25: 9, 27: 10, 28: 11, 31: 12, 32: 13, 33: 14, 34: 15, 35: 16, 36: 17, 37: 18, 38: 19, 39: 20, 40: 21, 41: 22, 42: 23, 43: 24, 46: 25, 47: 26, 48: 27, 49: 28, 50: 29, 51: 30, 52: 31, 53: 32, 54: 33, 55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 61: 40, 65: 41, 70: 42, 73: 43, 74: 44, 75: 45, 76: 46, 77: 47, 78: 48, 79: 49, 80: 50, 81: 51, 82: 52, 84: 53, 85: 54, 86: 55, 87: 56, 88: 57, 89: 58, 90: 59}
loading annotations into memory...
Done (t=0.43s)
creating index...
index created!
dataset_name : coco_val, image nums : 5000, anno nums : 35511
transformer.level_embed True
transformer.encoder.layers.0.self_attn.sampling_offsets.weight True
transformer.encoder.layers.0.self_attn.sampling_offsets.bias True
transformer.encoder.layers.0.self_attn.attention_weights.weight True
transformer.encoder.layers.0.self_attn.attention_weights.bias True
transformer.encoder.layers.0.self_attn.value_proj.weight True
transformer.encoder.layers.0.self_attn.value_proj.bias True
transformer.encoder.layers.0.self_attn.output_proj.weight True
transformer.encoder.layers.0.self_attn.output_proj.bias True
transformer.encoder.layers.0.norm1.weight True
transformer.encoder.layers.0.norm1.bias True
transformer.encoder.layers.0.linear1.weight True
transformer.encoder.layers.0.linear1.bias True
transformer.encoder.layers.0.linear2.weight True
transformer.encoder.layers.0.linear2.bias True
transformer.encoder.layers.0.norm2.weight True
transformer.encoder.layers.0.norm2.bias True
transformer.encoder.layers.1.self_attn.sampling_offsets.weight True
transformer.encoder.layers.1.self_attn.sampling_offsets.bias True
transformer.encoder.layers.1.self_attn.attention_weights.weight True
transformer.encoder.layers.1.self_attn.attention_weights.bias True
transformer.encoder.layers.1.self_attn.value_proj.weight True
transformer.encoder.layers.1.self_attn.value_proj.bias True
transformer.encoder.layers.1.self_attn.output_proj.weight True
transformer.encoder.layers.1.self_attn.output_proj.bias True
transformer.encoder.layers.1.norm1.weight True
transformer.encoder.layers.1.norm1.bias True
transformer.encoder.layers.1.linear1.weight True
transformer.encoder.layers.1.linear1.bias True
transformer.encoder.layers.1.linear2.weight True
transformer.encoder.layers.1.linear2.bias True
transformer.encoder.layers.1.norm2.weight True
transformer.encoder.layers.1.norm2.bias True
transformer.encoder.layers.2.self_attn.sampling_offsets.weight True
transformer.encoder.layers.2.self_attn.sampling_offsets.bias True
transformer.encoder.layers.2.self_attn.attention_weights.weight True
transformer.encoder.layers.2.self_attn.attention_weights.bias True
transformer.encoder.layers.2.self_attn.value_proj.weight True
transformer.encoder.layers.2.self_attn.value_proj.bias True
transformer.encoder.layers.2.self_attn.output_proj.weight True
transformer.encoder.layers.2.self_attn.output_proj.bias True
transformer.encoder.layers.2.norm1.weight True
transformer.encoder.layers.2.norm1.bias True
transformer.encoder.layers.2.linear1.weight True
transformer.encoder.layers.2.linear1.bias True
transformer.encoder.layers.2.linear2.weight True
transformer.encoder.layers.2.linear2.bias True
transformer.encoder.layers.2.norm2.weight True
transformer.encoder.layers.2.norm2.bias True
transformer.encoder.layers.3.self_attn.sampling_offsets.weight True
transformer.encoder.layers.3.self_attn.sampling_offsets.bias True
transformer.encoder.layers.3.self_attn.attention_weights.weight True
transformer.encoder.layers.3.self_attn.attention_weights.bias True
transformer.encoder.layers.3.self_attn.value_proj.weight True
transformer.encoder.layers.3.self_attn.value_proj.bias True
transformer.encoder.layers.3.self_attn.output_proj.weight True
transformer.encoder.layers.3.self_attn.output_proj.bias True
transformer.encoder.layers.3.norm1.weight True
transformer.encoder.layers.3.norm1.bias True
transformer.encoder.layers.3.linear1.weight True
transformer.encoder.layers.3.linear1.bias True
transformer.encoder.layers.3.linear2.weight True
transformer.encoder.layers.3.linear2.bias True
transformer.encoder.layers.3.norm2.weight True
transformer.encoder.layers.3.norm2.bias True
transformer.encoder.layers.4.self_attn.sampling_offsets.weight True
transformer.encoder.layers.4.self_attn.sampling_offsets.bias True
transformer.encoder.layers.4.self_attn.attention_weights.weight True
transformer.encoder.layers.4.self_attn.attention_weights.bias True
transformer.encoder.layers.4.self_attn.value_proj.weight True
transformer.encoder.layers.4.self_attn.value_proj.bias True
transformer.encoder.layers.4.self_attn.output_proj.weight True
transformer.encoder.layers.4.self_attn.output_proj.bias True
transformer.encoder.layers.4.norm1.weight True
transformer.encoder.layers.4.norm1.bias True
transformer.encoder.layers.4.linear1.weight True
transformer.encoder.layers.4.linear1.bias True
transformer.encoder.layers.4.linear2.weight True
transformer.encoder.layers.4.linear2.bias True
transformer.encoder.layers.4.norm2.weight True
transformer.encoder.layers.4.norm2.bias True
transformer.encoder.layers.5.self_attn.sampling_offsets.weight True
transformer.encoder.layers.5.self_attn.sampling_offsets.bias True
transformer.encoder.layers.5.self_attn.attention_weights.weight True
transformer.encoder.layers.5.self_attn.attention_weights.bias True
transformer.encoder.layers.5.self_attn.value_proj.weight True
transformer.encoder.layers.5.self_attn.value_proj.bias True
transformer.encoder.layers.5.self_attn.output_proj.weight True
transformer.encoder.layers.5.self_attn.output_proj.bias True
transformer.encoder.layers.5.norm1.weight True
transformer.encoder.layers.5.norm1.bias True
transformer.encoder.layers.5.linear1.weight True
transformer.encoder.layers.5.linear1.bias True
transformer.encoder.layers.5.linear2.weight True
transformer.encoder.layers.5.linear2.bias True
transformer.encoder.layers.5.norm2.weight True
transformer.encoder.layers.5.norm2.bias True
transformer.decoder.layers.0.cross_attn.sampling_offsets.weight True
transformer.decoder.layers.0.cross_attn.sampling_offsets.bias True
transformer.decoder.layers.0.cross_attn.attention_weights.weight True
transformer.decoder.layers.0.cross_attn.attention_weights.bias True
transformer.decoder.layers.0.cross_attn.value_proj.weight True
transformer.decoder.layers.0.cross_attn.value_proj.bias True
transformer.decoder.layers.0.cross_attn.output_proj.weight True
transformer.decoder.layers.0.cross_attn.output_proj.bias True
transformer.decoder.layers.0.norm1.weight True
transformer.decoder.layers.0.norm1.bias True
transformer.decoder.layers.0.self_attn.in_proj_weight True
transformer.decoder.layers.0.self_attn.in_proj_bias True
transformer.decoder.layers.0.self_attn.out_proj.weight True
transformer.decoder.layers.0.self_attn.out_proj.bias True
transformer.decoder.layers.0.norm2.weight True
transformer.decoder.layers.0.norm2.bias True
transformer.decoder.layers.0.linear1.weight True
transformer.decoder.layers.0.linear1.bias True
transformer.decoder.layers.0.linear2.weight True
transformer.decoder.layers.0.linear2.bias True
transformer.decoder.layers.0.norm3.weight True
transformer.decoder.layers.0.norm3.bias True
transformer.decoder.layers.1.cross_attn.sampling_offsets.weight True
transformer.decoder.layers.1.cross_attn.sampling_offsets.bias True
transformer.decoder.layers.1.cross_attn.attention_weights.weight True
transformer.decoder.layers.1.cross_attn.attention_weights.bias True
transformer.decoder.layers.1.cross_attn.value_proj.weight True
transformer.decoder.layers.1.cross_attn.value_proj.bias True
transformer.decoder.layers.1.cross_attn.output_proj.weight True
transformer.decoder.layers.1.cross_attn.output_proj.bias True
transformer.decoder.layers.1.norm1.weight True
transformer.decoder.layers.1.norm1.bias True
transformer.decoder.layers.1.self_attn.in_proj_weight True
transformer.decoder.layers.1.self_attn.in_proj_bias True
transformer.decoder.layers.1.self_attn.out_proj.weight True
transformer.decoder.layers.1.self_attn.out_proj.bias True
transformer.decoder.layers.1.norm2.weight True
transformer.decoder.layers.1.norm2.bias True
transformer.decoder.layers.1.linear1.weight True
transformer.decoder.layers.1.linear1.bias True
transformer.decoder.layers.1.linear2.weight True
transformer.decoder.layers.1.linear2.bias True
transformer.decoder.layers.1.norm3.weight True
transformer.decoder.layers.1.norm3.bias True
transformer.decoder.layers.2.cross_attn.sampling_offsets.weight True
transformer.decoder.layers.2.cross_attn.sampling_offsets.bias True
transformer.decoder.layers.2.cross_attn.attention_weights.weight True
transformer.decoder.layers.2.cross_attn.attention_weights.bias True
transformer.decoder.layers.2.cross_attn.value_proj.weight True
transformer.decoder.layers.2.cross_attn.value_proj.bias True
transformer.decoder.layers.2.cross_attn.output_proj.weight True
transformer.decoder.layers.2.cross_attn.output_proj.bias True
transformer.decoder.layers.2.norm1.weight True
transformer.decoder.layers.2.norm1.bias True
transformer.decoder.layers.2.self_attn.in_proj_weight True
transformer.decoder.layers.2.self_attn.in_proj_bias True
transformer.decoder.layers.2.self_attn.out_proj.weight True
transformer.decoder.layers.2.self_attn.out_proj.bias True
transformer.decoder.layers.2.norm2.weight True
transformer.decoder.layers.2.norm2.bias True
transformer.decoder.layers.2.linear1.weight True
transformer.decoder.layers.2.linear1.bias True
transformer.decoder.layers.2.linear2.weight True
transformer.decoder.layers.2.linear2.bias True
transformer.decoder.layers.2.norm3.weight True
transformer.decoder.layers.2.norm3.bias True
transformer.decoder.layers.3.cross_attn.sampling_offsets.weight True
transformer.decoder.layers.3.cross_attn.sampling_offsets.bias True
transformer.decoder.layers.3.cross_attn.attention_weights.weight True
transformer.decoder.layers.3.cross_attn.attention_weights.bias True
transformer.decoder.layers.3.cross_attn.value_proj.weight True
transformer.decoder.layers.3.cross_attn.value_proj.bias True
transformer.decoder.layers.3.cross_attn.output_proj.weight True
transformer.decoder.layers.3.cross_attn.output_proj.bias True
transformer.decoder.layers.3.norm1.weight True
transformer.decoder.layers.3.norm1.bias True
transformer.decoder.layers.3.self_attn.in_proj_weight True
transformer.decoder.layers.3.self_attn.in_proj_bias True
transformer.decoder.layers.3.self_attn.out_proj.weight True
transformer.decoder.layers.3.self_attn.out_proj.bias True
transformer.decoder.layers.3.norm2.weight True
transformer.decoder.layers.3.norm2.bias True
transformer.decoder.layers.3.linear1.weight True
transformer.decoder.layers.3.linear1.bias True
transformer.decoder.layers.3.linear2.weight True
transformer.decoder.layers.3.linear2.bias True
transformer.decoder.layers.3.norm3.weight True
transformer.decoder.layers.3.norm3.bias True
transformer.decoder.layers.4.cross_attn.sampling_offsets.weight True
transformer.decoder.layers.4.cross_attn.sampling_offsets.bias True
transformer.decoder.layers.4.cross_attn.attention_weights.weight True
transformer.decoder.layers.4.cross_attn.attention_weights.bias True
transformer.decoder.layers.4.cross_attn.value_proj.weight True
transformer.decoder.layers.4.cross_attn.value_proj.bias True
transformer.decoder.layers.4.cross_attn.output_proj.weight True
transformer.decoder.layers.4.cross_attn.output_proj.bias True
transformer.decoder.layers.4.norm1.weight True
transformer.decoder.layers.4.norm1.bias True
transformer.decoder.layers.4.self_attn.in_proj_weight True
transformer.decoder.layers.4.self_attn.in_proj_bias True
transformer.decoder.layers.4.self_attn.out_proj.weight True
transformer.decoder.layers.4.self_attn.out_proj.bias True
transformer.decoder.layers.4.norm2.weight True
transformer.decoder.layers.4.norm2.bias True
transformer.decoder.layers.4.linear1.weight True
transformer.decoder.layers.4.linear1.bias True
transformer.decoder.layers.4.linear2.weight True
transformer.decoder.layers.4.linear2.bias True
transformer.decoder.layers.4.norm3.weight True
transformer.decoder.layers.4.norm3.bias True
transformer.decoder.layers.5.cross_attn.sampling_offsets.weight True
transformer.decoder.layers.5.cross_attn.sampling_offsets.bias True
transformer.decoder.layers.5.cross_attn.attention_weights.weight True
transformer.decoder.layers.5.cross_attn.attention_weights.bias True
transformer.decoder.layers.5.cross_attn.value_proj.weight True
transformer.decoder.layers.5.cross_attn.value_proj.bias True
transformer.decoder.layers.5.cross_attn.output_proj.weight True
transformer.decoder.layers.5.cross_attn.output_proj.bias True
transformer.decoder.layers.5.norm1.weight True
transformer.decoder.layers.5.norm1.bias True
transformer.decoder.layers.5.self_attn.in_proj_weight True
transformer.decoder.layers.5.self_attn.in_proj_bias True
transformer.decoder.layers.5.self_attn.out_proj.weight True
transformer.decoder.layers.5.self_attn.out_proj.bias True
transformer.decoder.layers.5.norm2.weight True
transformer.decoder.layers.5.norm2.bias True
transformer.decoder.layers.5.linear1.weight True
transformer.decoder.layers.5.linear1.bias True
transformer.decoder.layers.5.linear2.weight True
transformer.decoder.layers.5.linear2.bias True
transformer.decoder.layers.5.norm3.weight True
transformer.decoder.layers.5.norm3.bias True
transformer.reference_points.weight True
transformer.reference_points.bias True
class_embed.0.weight True
class_embed.0.bias True
bbox_embed.0.layers.0.weight True
bbox_embed.0.layers.0.bias True
bbox_embed.0.layers.1.weight True
bbox_embed.0.layers.1.bias True
bbox_embed.0.layers.2.weight True
bbox_embed.0.layers.2.bias True
query_embed.weight True
input_proj.0.0.weight True
input_proj.0.0.bias True
input_proj.0.1.weight True
input_proj.0.1.bias True
input_proj.1.0.weight True
input_proj.1.0.bias True
input_proj.1.1.weight True
input_proj.1.1.bias True
input_proj.2.0.weight True
input_proj.2.0.bias True
input_proj.2.1.weight True
input_proj.2.1.bias True
input_proj.3.0.weight True
input_proj.3.0.bias True
input_proj.3.1.weight True
input_proj.3.1.bias True
backbone.0.body.conv1.weight False
backbone.0.body.layer1.0.conv1.weight False
backbone.0.body.layer1.0.conv2.weight False
backbone.0.body.layer1.0.conv3.weight False
backbone.0.body.layer1.0.downsample.0.weight False
backbone.0.body.layer1.1.conv1.weight False
backbone.0.body.layer1.1.conv2.weight False
backbone.0.body.layer1.1.conv3.weight False
backbone.0.body.layer1.2.conv1.weight False
backbone.0.body.layer1.2.conv2.weight False
backbone.0.body.layer1.2.conv3.weight False
backbone.0.body.layer2.0.conv1.weight True
backbone.0.body.layer2.0.conv2.weight True
backbone.0.body.layer2.0.conv3.weight True
backbone.0.body.layer2.0.downsample.0.weight True
backbone.0.body.layer2.1.conv1.weight True
backbone.0.body.layer2.1.conv2.weight True
backbone.0.body.layer2.1.conv3.weight True
backbone.0.body.layer2.2.conv1.weight True
backbone.0.body.layer2.2.conv2.weight True
backbone.0.body.layer2.2.conv3.weight True
backbone.0.body.layer2.3.conv1.weight True
backbone.0.body.layer2.3.conv2.weight True
backbone.0.body.layer2.3.conv3.weight True
backbone.0.body.layer3.0.conv1.weight True
backbone.0.body.layer3.0.conv2.weight True
backbone.0.body.layer3.0.conv3.weight True
backbone.0.body.layer3.0.downsample.0.weight True
backbone.0.body.layer3.1.conv1.weight True
backbone.0.body.layer3.1.conv2.weight True
backbone.0.body.layer3.1.conv3.weight True
backbone.0.body.layer3.2.conv1.weight True
backbone.0.body.layer3.2.conv2.weight True
backbone.0.body.layer3.2.conv3.weight True
backbone.0.body.layer3.3.conv1.weight True
backbone.0.body.layer3.3.conv2.weight True
backbone.0.body.layer3.3.conv3.weight True
backbone.0.body.layer3.4.conv1.weight True
backbone.0.body.layer3.4.conv2.weight True
backbone.0.body.layer3.4.conv3.weight True
backbone.0.body.layer3.5.conv1.weight True
backbone.0.body.layer3.5.conv2.weight True
backbone.0.body.layer3.5.conv3.weight True
backbone.0.body.layer4.0.conv1.weight True
backbone.0.body.layer4.0.conv2.weight True
backbone.0.body.layer4.0.conv3.weight True
backbone.0.body.layer4.0.downsample.0.weight True
backbone.0.body.layer4.1.conv1.weight True
backbone.0.body.layer4.1.conv2.weight True
backbone.0.body.layer4.1.conv3.weight True
backbone.0.body.layer4.2.conv1.weight True
backbone.0.body.layer4.2.conv2.weight True
backbone.0.body.layer4.2.conv3.weight True
data/coco/cocosplit_self/seed0/full_box_30shot_trainval.json
id map
{1: 60, 2: 61, 3: 62, 4: 63, 5: 64, 6: 65, 7: 66, 9: 67, 16: 68, 17: 69, 18: 70, 19: 71, 20: 72, 21: 73, 44: 74, 62: 75, 63: 76, 64: 77, 67: 78, 72: 79, 8: 0, 10: 1, 11: 2, 13: 3, 14: 4, 15: 5, 22: 6, 23: 7, 24: 8, 25: 9, 27: 10, 28: 11, 31: 12, 32: 13, 33: 14, 34: 15, 35: 16, 36: 17, 37: 18, 38: 19, 39: 20, 40: 21, 41: 22, 42: 23, 43: 24, 46: 25, 47: 26, 48: 27, 49: 28, 50: 29, 51: 30, 52: 31, 53: 32, 54: 33, 55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 61: 40, 65: 41, 70: 42, 73: 43, 74: 44, 75: 45, 76: 46, 77: 47, 78: 48, 79: 49, 80: 50, 81: 51, 82: 52, 84: 53, 85: 54, 86: 55, 87: 56, 88: 57, 89: 58, 90: 59}
loading annotations into memory...
Done (t=0.04s)
creating index...
index created!
dataset_name : coco_all_seed_0_30_shot_val, image nums : 1187, anno nums : 2394
start build prototypes ...
prototypes length: 80 
Test:  [  0/157]  eta: 0:08:33  class_error: 43.64  loss: 17.1070 (17.1070)  loss_bbox: 0.4883 (0.4883)  loss_bbox_0: 0.5446 (0.5446)  loss_bbox_1: 0.5239 (0.5239)  loss_bbox_2: 0.5246 (0.5246)  loss_bbox_3: 0.5074 (0.5074)  loss_bbox_4: 0.4896 (0.4896)  loss_ce: 1.1815 (1.1815)  loss_ce_0: 1.2070 (1.2070)  loss_ce_1: 1.2221 (1.2221)  loss_ce_2: 1.2101 (1.2101)  loss_ce_3: 1.2009 (1.2009)  loss_ce_4: 1.1887 (1.1887)  loss_giou: 1.1185 (1.1185)  loss_giou_0: 1.1767 (1.1767)  loss_giou_1: 1.1406 (1.1406)  loss_giou_2: 1.1370 (1.1370)  loss_giou_3: 1.1205 (1.1205)  loss_giou_4: 1.1251 (1.1251)  cardinality_error_unscaled: 89.2188 (89.2188)  cardinality_error_0_unscaled: 87.7500 (87.7500)  cardinality_error_1_unscaled: 88.7500 (88.7500)  cardinality_error_2_unscaled: 89.1562 (89.1562)  cardinality_error_3_unscaled: 88.7500 (88.7500)  cardinality_error_4_unscaled: 89.5625 (89.5625)  class_error_unscaled: 43.6381 (43.6381)  loss_bbox_unscaled: 0.0977 (0.0977)  loss_bbox_0_unscaled: 0.1089 (0.1089)  loss_bbox_1_unscaled: 0.1048 (0.1048)  loss_bbox_2_unscaled: 0.1049 (0.1049)  loss_bbox_3_unscaled: 0.1015 (0.1015)  loss_bbox_4_unscaled: 0.0979 (0.0979)  loss_ce_unscaled: 0.5907 (0.5907)  loss_ce_0_unscaled: 0.6035 (0.6035)  loss_ce_1_unscaled: 0.6111 (0.6111)  loss_ce_2_unscaled: 0.6050 (0.6050)  loss_ce_3_unscaled: 0.6005 (0.6005)  loss_ce_4_unscaled: 0.5943 (0.5943)  loss_giou_unscaled: 0.5592 (0.5592)  loss_giou_0_unscaled: 0.5884 (0.5884)  loss_giou_1_unscaled: 0.5703 (0.5703)  loss_giou_2_unscaled: 0.5685 (0.5685)  loss_giou_3_unscaled: 0.5602 (0.5602)  loss_giou_4_unscaled: 0.5625 (0.5625)  time: 3.2679  data: 0.5878  max mem: 2392
Test:  [ 10/157]  eta: 0:02:04  class_error: 34.02  loss: 17.1070 (16.8821)  loss_bbox: 0.5042 (0.5250)  loss_bbox_0: 0.5369 (0.5549)  loss_bbox_1: 0.5118 (0.5331)  loss_bbox_2: 0.5210 (0.5320)  loss_bbox_3: 0.5061 (0.5288)  loss_bbox_4: 0.5026 (0.5240)  loss_ce: 1.2476 (1.2496)  loss_ce_0: 1.2548 (1.2744)  loss_ce_1: 1.3047 (1.2954)  loss_ce_2: 1.2961 (1.2857)  loss_ce_3: 1.2817 (1.2750)  loss_ce_4: 1.2676 (1.2593)  loss_giou: 0.9950 (0.9999)  loss_giou_0: 1.0207 (1.0349)  loss_giou_1: 1.0038 (1.0090)  loss_giou_2: 0.9944 (1.0019)  loss_giou_3: 0.9937 (0.9986)  loss_giou_4: 0.9886 (1.0006)  cardinality_error_unscaled: 92.1250 (91.9403)  cardinality_error_0_unscaled: 91.1875 (90.7812)  cardinality_error_1_unscaled: 91.8750 (91.6733)  cardinality_error_2_unscaled: 92.0312 (91.8011)  cardinality_error_3_unscaled: 92.0312 (91.6932)  cardinality_error_4_unscaled: 92.2500 (92.0170)  class_error_unscaled: 42.0286 (41.3828)  loss_bbox_unscaled: 0.1008 (0.1050)  loss_bbox_0_unscaled: 0.1074 (0.1110)  loss_bbox_1_unscaled: 0.1024 (0.1066)  loss_bbox_2_unscaled: 0.1042 (0.1064)  loss_bbox_3_unscaled: 0.1012 (0.1058)  loss_bbox_4_unscaled: 0.1005 (0.1048)  loss_ce_unscaled: 0.6238 (0.6248)  loss_ce_0_unscaled: 0.6274 (0.6372)  loss_ce_1_unscaled: 0.6524 (0.6477)  loss_ce_2_unscaled: 0.6480 (0.6428)  loss_ce_3_unscaled: 0.6409 (0.6375)  loss_ce_4_unscaled: 0.6338 (0.6297)  loss_giou_unscaled: 0.4975 (0.5000)  loss_giou_0_unscaled: 0.5103 (0.5175)  loss_giou_1_unscaled: 0.5019 (0.5045)  loss_giou_2_unscaled: 0.4972 (0.5010)  loss_giou_3_unscaled: 0.4969 (0.4993)  loss_giou_4_unscaled: 0.4943 (0.5003)  time: 0.8494  data: 0.0629  max mem: 2652
Test:  [ 20/157]  eta: 0:01:40  class_error: 40.38  loss: 16.5404 (16.8669)  loss_bbox: 0.5198 (0.5339)  loss_bbox_0: 0.5394 (0.5649)  loss_bbox_1: 0.5149 (0.5447)  loss_bbox_2: 0.5316 (0.5420)  loss_bbox_3: 0.5125 (0.5367)  loss_bbox_4: 0.5213 (0.5346)  loss_ce: 1.2619 (1.2612)  loss_ce_0: 1.2644 (1.2861)  loss_ce_1: 1.2899 (1.3013)  loss_ce_2: 1.2804 (1.2909)  loss_ce_3: 1.2808 (1.2816)  loss_ce_4: 1.2591 (1.2672)  loss_giou: 0.9695 (0.9763)  loss_giou_0: 1.0207 (1.0168)  loss_giou_1: 0.9859 (0.9920)  loss_giou_2: 0.9746 (0.9820)  loss_giou_3: 0.9644 (0.9781)  loss_giou_4: 0.9719 (0.9767)  cardinality_error_unscaled: 92.5000 (92.2708)  cardinality_error_0_unscaled: 91.5312 (91.2039)  cardinality_error_1_unscaled: 92.4688 (92.0967)  cardinality_error_2_unscaled: 92.5000 (92.1935)  cardinality_error_3_unscaled: 92.2500 (92.0759)  cardinality_error_4_unscaled: 92.6562 (92.3661)  class_error_unscaled: 42.0286 (43.2652)  loss_bbox_unscaled: 0.1040 (0.1068)  loss_bbox_0_unscaled: 0.1079 (0.1130)  loss_bbox_1_unscaled: 0.1030 (0.1089)  loss_bbox_2_unscaled: 0.1063 (0.1084)  loss_bbox_3_unscaled: 0.1025 (0.1073)  loss_bbox_4_unscaled: 0.1043 (0.1069)  loss_ce_unscaled: 0.6310 (0.6306)  loss_ce_0_unscaled: 0.6322 (0.6430)  loss_ce_1_unscaled: 0.6450 (0.6507)  loss_ce_2_unscaled: 0.6402 (0.6455)  loss_ce_3_unscaled: 0.6404 (0.6408)  loss_ce_4_unscaled: 0.6295 (0.6336)  loss_giou_unscaled: 0.4848 (0.4881)  loss_giou_0_unscaled: 0.5103 (0.5084)  loss_giou_1_unscaled: 0.4930 (0.4960)  loss_giou_2_unscaled: 0.4873 (0.4910)  loss_giou_3_unscaled: 0.4822 (0.4890)  loss_giou_4_unscaled: 0.4859 (0.4883)  time: 0.6038  data: 0.0093  max mem: 2703
Test:  [ 30/157]  eta: 0:01:28  class_error: 47.74  loss: 16.4492 (16.7890)  loss_bbox: 0.4982 (0.5202)  loss_bbox_0: 0.5328 (0.5520)  loss_bbox_1: 0.5148 (0.5329)  loss_bbox_2: 0.4998 (0.5265)  loss_bbox_3: 0.5000 (0.5228)  loss_bbox_4: 0.5010 (0.5216)  loss_ce: 1.2621 (1.2620)  loss_ce_0: 1.2785 (1.2855)  loss_ce_1: 1.2899 (1.3050)  loss_ce_2: 1.2789 (1.2949)  loss_ce_3: 1.2776 (1.2847)  loss_ce_4: 1.2591 (1.2667)  loss_giou: 0.9733 (0.9739)  loss_giou_0: 1.0238 (1.0174)  loss_giou_1: 0.9859 (0.9895)  loss_giou_2: 0.9733 (0.9809)  loss_giou_3: 0.9613 (0.9766)  loss_giou_4: 0.9632 (0.9758)  cardinality_error_unscaled: 92.1875 (91.9889)  cardinality_error_0_unscaled: 90.8438 (90.8377)  cardinality_error_1_unscaled: 91.8750 (91.7440)  cardinality_error_2_unscaled: 92.0312 (91.9214)  cardinality_error_3_unscaled: 91.9375 (91.7772)  cardinality_error_4_unscaled: 92.2188 (92.0887)  class_error_unscaled: 44.9361 (44.0514)  loss_bbox_unscaled: 0.0996 (0.1040)  loss_bbox_0_unscaled: 0.1066 (0.1104)  loss_bbox_1_unscaled: 0.1030 (0.1066)  loss_bbox_2_unscaled: 0.1000 (0.1053)  loss_bbox_3_unscaled: 0.1000 (0.1046)  loss_bbox_4_unscaled: 0.1002 (0.1043)  loss_ce_unscaled: 0.6311 (0.6310)  loss_ce_0_unscaled: 0.6393 (0.6428)  loss_ce_1_unscaled: 0.6450 (0.6525)  loss_ce_2_unscaled: 0.6394 (0.6474)  loss_ce_3_unscaled: 0.6388 (0.6423)  loss_ce_4_unscaled: 0.6295 (0.6334)  loss_giou_unscaled: 0.4867 (0.4869)  loss_giou_0_unscaled: 0.5119 (0.5087)  loss_giou_1_unscaled: 0.4930 (0.4948)  loss_giou_2_unscaled: 0.4866 (0.4904)  loss_giou_3_unscaled: 0.4806 (0.4883)  loss_giou_4_unscaled: 0.4816 (0.4879)  time: 0.6100  data: 0.0083  max mem: 2880
Test:  [ 40/157]  eta: 0:01:19  class_error: 47.47  loss: 17.1793 (17.2034)  loss_bbox: 0.5348 (0.5320)  loss_bbox_0: 0.5843 (0.5651)  loss_bbox_1: 0.5392 (0.5467)  loss_bbox_2: 0.5358 (0.5394)  loss_bbox_3: 0.5359 (0.5350)  loss_bbox_4: 0.5355 (0.5342)  loss_ce: 1.3398 (1.2933)  loss_ce_0: 1.3475 (1.3129)  loss_ce_1: 1.3822 (1.3362)  loss_ce_2: 1.3905 (1.3275)  loss_ce_3: 1.3729 (1.3156)  loss_ce_4: 1.3426 (1.2972)  loss_giou: 1.0280 (1.0007)  loss_giou_0: 1.0687 (1.0423)  loss_giou_1: 1.0465 (1.0151)  loss_giou_2: 1.0341 (1.0056)  loss_giou_3: 1.0312 (1.0027)  loss_giou_4: 1.0258 (1.0019)  cardinality_error_unscaled: 91.9062 (91.9787)  cardinality_error_0_unscaled: 90.8125 (90.8887)  cardinality_error_1_unscaled: 91.5000 (91.7424)  cardinality_error_2_unscaled: 91.9062 (91.9207)  cardinality_error_3_unscaled: 91.7188 (91.7912)  cardinality_error_4_unscaled: 91.9688 (92.0678)  class_error_unscaled: 47.1627 (45.1790)  loss_bbox_unscaled: 0.1070 (0.1064)  loss_bbox_0_unscaled: 0.1169 (0.1130)  loss_bbox_1_unscaled: 0.1078 (0.1093)  loss_bbox_2_unscaled: 0.1072 (0.1079)  loss_bbox_3_unscaled: 0.1072 (0.1070)  loss_bbox_4_unscaled: 0.1071 (0.1068)  loss_ce_unscaled: 0.6699 (0.6466)  loss_ce_0_unscaled: 0.6738 (0.6564)  loss_ce_1_unscaled: 0.6911 (0.6681)  loss_ce_2_unscaled: 0.6952 (0.6638)  loss_ce_3_unscaled: 0.6865 (0.6578)  loss_ce_4_unscaled: 0.6713 (0.6486)  loss_giou_unscaled: 0.5140 (0.5003)  loss_giou_0_unscaled: 0.5343 (0.5211)  loss_giou_1_unscaled: 0.5232 (0.5075)  loss_giou_2_unscaled: 0.5171 (0.5028)  loss_giou_3_unscaled: 0.5156 (0.5013)  loss_giou_4_unscaled: 0.5129 (0.5009)  time: 0.6280  data: 0.0086  max mem: 2979
Test:  [ 50/157]  eta: 0:01:11  class_error: 41.94  loss: 17.1793 (17.0006)  loss_bbox: 0.5452 (0.5274)  loss_bbox_0: 0.5856 (0.5599)  loss_bbox_1: 0.5533 (0.5430)  loss_bbox_2: 0.5461 (0.5360)  loss_bbox_3: 0.5391 (0.5310)  loss_bbox_4: 0.5398 (0.5294)  loss_ce: 1.2993 (1.2743)  loss_ce_0: 1.2989 (1.2933)  loss_ce_1: 1.3465 (1.3160)  loss_ce_2: 1.3361 (1.3071)  loss_ce_3: 1.3254 (1.2950)  loss_ce_4: 1.3003 (1.2784)  loss_giou: 0.9791 (0.9907)  loss_giou_0: 1.0290 (1.0327)  loss_giou_1: 0.9952 (1.0055)  loss_giou_2: 0.9957 (0.9956)  loss_giou_3: 0.9870 (0.9935)  loss_giou_4: 0.9883 (0.9916)  cardinality_error_unscaled: 92.5938 (92.1140)  cardinality_error_0_unscaled: 91.6250 (91.0882)  cardinality_error_1_unscaled: 92.3750 (91.8915)  cardinality_error_2_unscaled: 92.5938 (92.0754)  cardinality_error_3_unscaled: 92.5938 (91.9528)  cardinality_error_4_unscaled: 92.7188 (92.2010)  class_error_unscaled: 42.7186 (44.2004)  loss_bbox_unscaled: 0.1090 (0.1055)  loss_bbox_0_unscaled: 0.1171 (0.1120)  loss_bbox_1_unscaled: 0.1107 (0.1086)  loss_bbox_2_unscaled: 0.1092 (0.1072)  loss_bbox_3_unscaled: 0.1078 (0.1062)  loss_bbox_4_unscaled: 0.1080 (0.1059)  loss_ce_unscaled: 0.6496 (0.6371)  loss_ce_0_unscaled: 0.6495 (0.6467)  loss_ce_1_unscaled: 0.6732 (0.6580)  loss_ce_2_unscaled: 0.6680 (0.6536)  loss_ce_3_unscaled: 0.6627 (0.6475)  loss_ce_4_unscaled: 0.6502 (0.6392)  loss_giou_unscaled: 0.4896 (0.4954)  loss_giou_0_unscaled: 0.5145 (0.5164)  loss_giou_1_unscaled: 0.4976 (0.5027)  loss_giou_2_unscaled: 0.4978 (0.4978)  loss_giou_3_unscaled: 0.4935 (0.4968)  loss_giou_4_unscaled: 0.4942 (0.4958)  time: 0.6245  data: 0.0090  max mem: 2979
Test:  [ 60/157]  eta: 0:01:03  class_error: 44.40  loss: 16.2584 (17.0180)  loss_bbox: 0.5014 (0.5249)  loss_bbox_0: 0.5365 (0.5579)  loss_bbox_1: 0.5323 (0.5409)  loss_bbox_2: 0.5123 (0.5335)  loss_bbox_3: 0.5074 (0.5282)  loss_bbox_4: 0.4970 (0.5268)  loss_ce: 1.2468 (1.2716)  loss_ce_0: 1.2377 (1.2903)  loss_ce_1: 1.2789 (1.3127)  loss_ce_2: 1.2683 (1.3037)  loss_ce_3: 1.2553 (1.2924)  loss_ce_4: 1.2419 (1.2753)  loss_giou: 0.9525 (0.9995)  loss_giou_0: 1.0086 (1.0408)  loss_giou_1: 0.9570 (1.0134)  loss_giou_2: 0.9609 (1.0039)  loss_giou_3: 0.9593 (1.0021)  loss_giou_4: 0.9600 (1.0002)  cardinality_error_unscaled: 92.5000 (92.0881)  cardinality_error_0_unscaled: 91.5312 (91.0815)  cardinality_error_1_unscaled: 92.1875 (91.8776)  cardinality_error_2_unscaled: 92.2812 (92.0487)  cardinality_error_3_unscaled: 92.3125 (91.9339)  cardinality_error_4_unscaled: 92.5625 (92.1844)  class_error_unscaled: 41.9354 (44.3533)  loss_bbox_unscaled: 0.1003 (0.1050)  loss_bbox_0_unscaled: 0.1073 (0.1116)  loss_bbox_1_unscaled: 0.1065 (0.1082)  loss_bbox_2_unscaled: 0.1025 (0.1067)  loss_bbox_3_unscaled: 0.1015 (0.1056)  loss_bbox_4_unscaled: 0.0994 (0.1054)  loss_ce_unscaled: 0.6234 (0.6358)  loss_ce_0_unscaled: 0.6189 (0.6451)  loss_ce_1_unscaled: 0.6395 (0.6563)  loss_ce_2_unscaled: 0.6342 (0.6519)  loss_ce_3_unscaled: 0.6276 (0.6462)  loss_ce_4_unscaled: 0.6210 (0.6376)  loss_giou_unscaled: 0.4763 (0.4997)  loss_giou_0_unscaled: 0.5043 (0.5204)  loss_giou_1_unscaled: 0.4785 (0.5067)  loss_giou_2_unscaled: 0.4805 (0.5019)  loss_giou_3_unscaled: 0.4797 (0.5010)  loss_giou_4_unscaled: 0.4800 (0.5001)  time: 0.6171  data: 0.0094  max mem: 2979
Test:  [ 70/157]  eta: 0:00:57  class_error: 34.57  loss: 16.6349 (17.0312)  loss_bbox: 0.5166 (0.5244)  loss_bbox_0: 0.5615 (0.5578)  loss_bbox_1: 0.5380 (0.5411)  loss_bbox_2: 0.5318 (0.5330)  loss_bbox_3: 0.5241 (0.5280)  loss_bbox_4: 0.5228 (0.5268)  loss_ce: 1.2468 (1.2680)  loss_ce_0: 1.2650 (1.2879)  loss_ce_1: 1.2789 (1.3105)  loss_ce_2: 1.2683 (1.3008)  loss_ce_3: 1.2553 (1.2885)  loss_ce_4: 1.2419 (1.2711)  loss_giou: 1.0237 (1.0046)  loss_giou_0: 1.0499 (1.0463)  loss_giou_1: 1.0450 (1.0190)  loss_giou_2: 1.0284 (1.0095)  loss_giou_3: 1.0181 (1.0079)  loss_giou_4: 1.0246 (1.0060)  cardinality_error_unscaled: 91.4688 (92.0339)  cardinality_error_0_unscaled: 90.7812 (91.0480)  cardinality_error_1_unscaled: 91.4375 (91.8345)  cardinality_error_2_unscaled: 91.5000 (92.0088)  cardinality_error_3_unscaled: 91.4062 (91.8895)  cardinality_error_4_unscaled: 91.6562 (92.1329)  class_error_unscaled: 44.1373 (44.0886)  loss_bbox_unscaled: 0.1033 (0.1049)  loss_bbox_0_unscaled: 0.1123 (0.1116)  loss_bbox_1_unscaled: 0.1076 (0.1082)  loss_bbox_2_unscaled: 0.1064 (0.1066)  loss_bbox_3_unscaled: 0.1048 (0.1056)  loss_bbox_4_unscaled: 0.1046 (0.1054)  loss_ce_unscaled: 0.6234 (0.6340)  loss_ce_0_unscaled: 0.6325 (0.6440)  loss_ce_1_unscaled: 0.6395 (0.6552)  loss_ce_2_unscaled: 0.6342 (0.6504)  loss_ce_3_unscaled: 0.6276 (0.6442)  loss_ce_4_unscaled: 0.6210 (0.6356)  loss_giou_unscaled: 0.5119 (0.5023)  loss_giou_0_unscaled: 0.5249 (0.5232)  loss_giou_1_unscaled: 0.5225 (0.5095)  loss_giou_2_unscaled: 0.5142 (0.5047)  loss_giou_3_unscaled: 0.5090 (0.5039)  loss_giou_4_unscaled: 0.5123 (0.5030)  time: 0.6380  data: 0.0103  max mem: 3135
Test:  [ 80/157]  eta: 0:00:50  class_error: 41.19  loss: 17.5044 (17.0695)  loss_bbox: 0.5533 (0.5279)  loss_bbox_0: 0.5899 (0.5613)  loss_bbox_1: 0.5786 (0.5446)  loss_bbox_2: 0.5532 (0.5365)  loss_bbox_3: 0.5487 (0.5313)  loss_bbox_4: 0.5512 (0.5302)  loss_ce: 1.2846 (1.2695)  loss_ce_0: 1.3154 (1.2892)  loss_ce_1: 1.3490 (1.3111)  loss_ce_2: 1.3246 (1.3018)  loss_ce_3: 1.3023 (1.2900)  loss_ce_4: 1.2890 (1.2727)  loss_giou: 1.0280 (1.0057)  loss_giou_0: 1.0613 (1.0480)  loss_giou_1: 1.0367 (1.0210)  loss_giou_2: 1.0284 (1.0118)  loss_giou_3: 1.0217 (1.0094)  loss_giou_4: 1.0246 (1.0075)  cardinality_error_unscaled: 91.7500 (92.0691)  cardinality_error_0_unscaled: 90.8750 (91.0802)  cardinality_error_1_unscaled: 91.6875 (91.8630)  cardinality_error_2_unscaled: 91.8750 (92.0440)  cardinality_error_3_unscaled: 91.6875 (91.9298)  cardinality_error_4_unscaled: 91.9688 (92.1767)  class_error_unscaled: 42.4785 (43.7651)  loss_bbox_unscaled: 0.1107 (0.1056)  loss_bbox_0_unscaled: 0.1180 (0.1123)  loss_bbox_1_unscaled: 0.1157 (0.1089)  loss_bbox_2_unscaled: 0.1106 (0.1073)  loss_bbox_3_unscaled: 0.1097 (0.1063)  loss_bbox_4_unscaled: 0.1102 (0.1060)  loss_ce_unscaled: 0.6423 (0.6347)  loss_ce_0_unscaled: 0.6577 (0.6446)  loss_ce_1_unscaled: 0.6745 (0.6556)  loss_ce_2_unscaled: 0.6623 (0.6509)  loss_ce_3_unscaled: 0.6511 (0.6450)  loss_ce_4_unscaled: 0.6445 (0.6363)  loss_giou_unscaled: 0.5140 (0.5029)  loss_giou_0_unscaled: 0.5306 (0.5240)  loss_giou_1_unscaled: 0.5184 (0.5105)  loss_giou_2_unscaled: 0.5142 (0.5059)  loss_giou_3_unscaled: 0.5108 (0.5047)  loss_giou_4_unscaled: 0.5123 (0.5038)  time: 0.6368  data: 0.0106  max mem: 3135
Test:  [ 90/157]  eta: 0:00:44  class_error: 45.97  loss: 17.1210 (17.0619)  loss_bbox: 0.5554 (0.5314)  loss_bbox_0: 0.5764 (0.5639)  loss_bbox_1: 0.5786 (0.5479)  loss_bbox_2: 0.5566 (0.5397)  loss_bbox_3: 0.5575 (0.5346)  loss_bbox_4: 0.5585 (0.5334)  loss_ce: 1.2546 (1.2664)  loss_ce_0: 1.2963 (1.2866)  loss_ce_1: 1.2845 (1.3077)  loss_ce_2: 1.2950 (1.2987)  loss_ce_3: 1.2802 (1.2872)  loss_ce_4: 1.2592 (1.2694)  loss_giou: 0.9655 (1.0041)  loss_giou_0: 1.0281 (1.0464)  loss_giou_1: 0.9998 (1.0199)  loss_giou_2: 0.9875 (1.0109)  loss_giou_3: 0.9767 (1.0077)  loss_giou_4: 0.9851 (1.0060)  cardinality_error_unscaled: 91.9688 (92.0766)  cardinality_error_0_unscaled: 90.9688 (91.1133)  cardinality_error_1_unscaled: 91.7500 (91.8764)  cardinality_error_2_unscaled: 92.0312 (92.0532)  cardinality_error_3_unscaled: 91.9062 (91.9451)  cardinality_error_4_unscaled: 92.2812 (92.1878)  class_error_unscaled: 41.1910 (43.5354)  loss_bbox_unscaled: 0.1111 (0.1063)  loss_bbox_0_unscaled: 0.1153 (0.1128)  loss_bbox_1_unscaled: 0.1157 (0.1096)  loss_bbox_2_unscaled: 0.1113 (0.1079)  loss_bbox_3_unscaled: 0.1115 (0.1069)  loss_bbox_4_unscaled: 0.1117 (0.1067)  loss_ce_unscaled: 0.6273 (0.6332)  loss_ce_0_unscaled: 0.6481 (0.6433)  loss_ce_1_unscaled: 0.6423 (0.6538)  loss_ce_2_unscaled: 0.6475 (0.6493)  loss_ce_3_unscaled: 0.6401 (0.6436)  loss_ce_4_unscaled: 0.6296 (0.6347)  loss_giou_unscaled: 0.4827 (0.5021)  loss_giou_0_unscaled: 0.5140 (0.5232)  loss_giou_1_unscaled: 0.4999 (0.5100)  loss_giou_2_unscaled: 0.4937 (0.5055)  loss_giou_3_unscaled: 0.4883 (0.5038)  loss_giou_4_unscaled: 0.4925 (0.5030)  time: 0.6617  data: 0.0098  max mem: 3135
Test:  [100/157]  eta: 0:00:37  class_error: 29.62  loss: 16.4851 (16.9875)  loss_bbox: 0.5423 (0.5302)  loss_bbox_0: 0.5687 (0.5631)  loss_bbox_1: 0.5629 (0.5472)  loss_bbox_2: 0.5525 (0.5386)  loss_bbox_3: 0.5519 (0.5336)  loss_bbox_4: 0.5456 (0.5324)  loss_ce: 1.2201 (1.2627)  loss_ce_0: 1.2503 (1.2813)  loss_ce_1: 1.2744 (1.3033)  loss_ce_2: 1.2425 (1.2948)  loss_ce_3: 1.2449 (1.2828)  loss_ce_4: 1.2218 (1.2658)  loss_giou: 0.9425 (0.9965)  loss_giou_0: 0.9930 (1.0391)  loss_giou_1: 0.9747 (1.0127)  loss_giou_2: 0.9694 (1.0039)  loss_giou_3: 0.9530 (1.0006)  loss_giou_4: 0.9444 (0.9988)  cardinality_error_unscaled: 92.5625 (92.1389)  cardinality_error_0_unscaled: 91.2500 (91.1627)  cardinality_error_1_unscaled: 92.1250 (91.9208)  cardinality_error_2_unscaled: 92.5625 (92.1108)  cardinality_error_3_unscaled: 92.4688 (91.9978)  cardinality_error_4_unscaled: 92.6875 (92.2550)  class_error_unscaled: 40.4165 (43.3009)  loss_bbox_unscaled: 0.1085 (0.1060)  loss_bbox_0_unscaled: 0.1137 (0.1126)  loss_bbox_1_unscaled: 0.1126 (0.1094)  loss_bbox_2_unscaled: 0.1105 (0.1077)  loss_bbox_3_unscaled: 0.1104 (0.1067)  loss_bbox_4_unscaled: 0.1091 (0.1065)  loss_ce_unscaled: 0.6101 (0.6314)  loss_ce_0_unscaled: 0.6251 (0.6407)  loss_ce_1_unscaled: 0.6372 (0.6516)  loss_ce_2_unscaled: 0.6212 (0.6474)  loss_ce_3_unscaled: 0.6225 (0.6414)  loss_ce_4_unscaled: 0.6109 (0.6329)  loss_giou_unscaled: 0.4713 (0.4982)  loss_giou_0_unscaled: 0.4965 (0.5195)  loss_giou_1_unscaled: 0.4873 (0.5063)  loss_giou_2_unscaled: 0.4847 (0.5020)  loss_giou_3_unscaled: 0.4765 (0.5003)  loss_giou_4_unscaled: 0.4722 (0.4994)  time: 0.6549  data: 0.0091  max mem: 3135
Test:  [110/157]  eta: 0:00:30  class_error: 42.49  loss: 16.7029 (16.9831)  loss_bbox: 0.5155 (0.5297)  loss_bbox_0: 0.5436 (0.5615)  loss_bbox_1: 0.5474 (0.5459)  loss_bbox_2: 0.5276 (0.5381)  loss_bbox_3: 0.5328 (0.5332)  loss_bbox_4: 0.5228 (0.5320)  loss_ce: 1.2092 (1.2623)  loss_ce_0: 1.2687 (1.2811)  loss_ce_1: 1.2640 (1.3022)  loss_ce_2: 1.2337 (1.2937)  loss_ce_3: 1.2280 (1.2818)  loss_ce_4: 1.2123 (1.2655)  loss_giou: 0.9918 (0.9975)  loss_giou_0: 0.9959 (1.0386)  loss_giou_1: 1.0070 (1.0135)  loss_giou_2: 0.9999 (1.0056)  loss_giou_3: 0.9825 (1.0014)  loss_giou_4: 0.9858 (0.9997)  cardinality_error_unscaled: 92.8125 (92.2055)  cardinality_error_0_unscaled: 92.0625 (91.2449)  cardinality_error_1_unscaled: 92.5000 (91.9862)  cardinality_error_2_unscaled: 93.0000 (92.1810)  cardinality_error_3_unscaled: 92.8125 (92.0707)  cardinality_error_4_unscaled: 93.0000 (92.3170)  class_error_unscaled: 41.2547 (43.2438)  loss_bbox_unscaled: 0.1031 (0.1059)  loss_bbox_0_unscaled: 0.1087 (0.1123)  loss_bbox_1_unscaled: 0.1095 (0.1092)  loss_bbox_2_unscaled: 0.1055 (0.1076)  loss_bbox_3_unscaled: 0.1066 (0.1066)  loss_bbox_4_unscaled: 0.1046 (0.1064)  loss_ce_unscaled: 0.6046 (0.6312)  loss_ce_0_unscaled: 0.6344 (0.6405)  loss_ce_1_unscaled: 0.6320 (0.6511)  loss_ce_2_unscaled: 0.6168 (0.6469)  loss_ce_3_unscaled: 0.6140 (0.6409)  loss_ce_4_unscaled: 0.6062 (0.6327)  loss_giou_unscaled: 0.4959 (0.4988)  loss_giou_0_unscaled: 0.4980 (0.5193)  loss_giou_1_unscaled: 0.5035 (0.5067)  loss_giou_2_unscaled: 0.4999 (0.5028)  loss_giou_3_unscaled: 0.4913 (0.5007)  loss_giou_4_unscaled: 0.4929 (0.4998)  time: 0.6059  data: 0.0092  max mem: 3135
Test:  [120/157]  eta: 0:00:23  class_error: 49.11  loss: 17.0158 (17.0056)  loss_bbox: 0.5153 (0.5321)  loss_bbox_0: 0.5416 (0.5636)  loss_bbox_1: 0.5355 (0.5479)  loss_bbox_2: 0.5276 (0.5402)  loss_bbox_3: 0.5260 (0.5355)  loss_bbox_4: 0.5181 (0.5340)  loss_ce: 1.2828 (1.2657)  loss_ce_0: 1.2708 (1.2836)  loss_ce_1: 1.2858 (1.3050)  loss_ce_2: 1.2956 (1.2972)  loss_ce_3: 1.2817 (1.2854)  loss_ce_4: 1.2769 (1.2689)  loss_giou: 0.9935 (0.9957)  loss_giou_0: 1.0109 (1.0374)  loss_giou_1: 1.0190 (1.0122)  loss_giou_2: 1.0042 (1.0037)  loss_giou_3: 0.9910 (0.9994)  loss_giou_4: 0.9871 (0.9979)  cardinality_error_unscaled: 92.4688 (92.1865)  cardinality_error_0_unscaled: 91.2812 (91.2461)  cardinality_error_1_unscaled: 91.8750 (91.9783)  cardinality_error_2_unscaled: 92.6250 (92.1772)  cardinality_error_3_unscaled: 92.4375 (92.0633)  cardinality_error_4_unscaled: 92.6875 (92.2978)  class_error_unscaled: 43.8223 (43.4754)  loss_bbox_unscaled: 0.1031 (0.1064)  loss_bbox_0_unscaled: 0.1083 (0.1127)  loss_bbox_1_unscaled: 0.1071 (0.1096)  loss_bbox_2_unscaled: 0.1055 (0.1080)  loss_bbox_3_unscaled: 0.1052 (0.1071)  loss_bbox_4_unscaled: 0.1036 (0.1068)  loss_ce_unscaled: 0.6414 (0.6328)  loss_ce_0_unscaled: 0.6354 (0.6418)  loss_ce_1_unscaled: 0.6429 (0.6525)  loss_ce_2_unscaled: 0.6478 (0.6486)  loss_ce_3_unscaled: 0.6409 (0.6427)  loss_ce_4_unscaled: 0.6385 (0.6345)  loss_giou_unscaled: 0.4967 (0.4978)  loss_giou_0_unscaled: 0.5054 (0.5187)  loss_giou_1_unscaled: 0.5095 (0.5061)  loss_giou_2_unscaled: 0.5021 (0.5019)  loss_giou_3_unscaled: 0.4955 (0.4997)  loss_giou_4_unscaled: 0.4936 (0.4990)  time: 0.6187  data: 0.0092  max mem: 3135
Test:  [130/157]  eta: 0:00:17  class_error: 48.80  loss: 16.5896 (16.9450)  loss_bbox: 0.5149 (0.5296)  loss_bbox_0: 0.5626 (0.5614)  loss_bbox_1: 0.5355 (0.5454)  loss_bbox_2: 0.5276 (0.5375)  loss_bbox_3: 0.5246 (0.5328)  loss_bbox_4: 0.5202 (0.5316)  loss_ce: 1.2116 (1.2620)  loss_ce_0: 1.2373 (1.2801)  loss_ce_1: 1.2688 (1.3010)  loss_ce_2: 1.2547 (1.2934)  loss_ce_3: 1.2384 (1.2818)  loss_ce_4: 1.2210 (1.2653)  loss_giou: 0.9565 (0.9916)  loss_giou_0: 1.0021 (1.0344)  loss_giou_1: 0.9752 (1.0082)  loss_giou_2: 0.9649 (0.9998)  loss_giou_3: 0.9518 (0.9954)  loss_giou_4: 0.9572 (0.9937)  cardinality_error_unscaled: 91.7188 (92.1832)  cardinality_error_0_unscaled: 90.9688 (91.2505)  cardinality_error_1_unscaled: 91.5312 (91.9745)  cardinality_error_2_unscaled: 91.6562 (92.1698)  cardinality_error_3_unscaled: 91.5625 (92.0606)  cardinality_error_4_unscaled: 91.8125 (92.2915)  class_error_unscaled: 43.8223 (43.3467)  loss_bbox_unscaled: 0.1030 (0.1059)  loss_bbox_0_unscaled: 0.1125 (0.1123)  loss_bbox_1_unscaled: 0.1071 (0.1091)  loss_bbox_2_unscaled: 0.1055 (0.1075)  loss_bbox_3_unscaled: 0.1049 (0.1066)  loss_bbox_4_unscaled: 0.1040 (0.1063)  loss_ce_unscaled: 0.6058 (0.6310)  loss_ce_0_unscaled: 0.6187 (0.6400)  loss_ce_1_unscaled: 0.6344 (0.6505)  loss_ce_2_unscaled: 0.6274 (0.6467)  loss_ce_3_unscaled: 0.6192 (0.6409)  loss_ce_4_unscaled: 0.6105 (0.6327)  loss_giou_unscaled: 0.4782 (0.4958)  loss_giou_0_unscaled: 0.5010 (0.5172)  loss_giou_1_unscaled: 0.4876 (0.5041)  loss_giou_2_unscaled: 0.4825 (0.4999)  loss_giou_3_unscaled: 0.4759 (0.4977)  loss_giou_4_unscaled: 0.4786 (0.4969)  time: 0.6292  data: 0.0089  max mem: 3135
Test:  [140/157]  eta: 0:00:10  class_error: 28.79  loss: 16.7658 (16.9470)  loss_bbox: 0.5553 (0.5306)  loss_bbox_0: 0.5728 (0.5623)  loss_bbox_1: 0.5511 (0.5463)  loss_bbox_2: 0.5458 (0.5385)  loss_bbox_3: 0.5512 (0.5340)  loss_bbox_4: 0.5463 (0.5324)  loss_ce: 1.2033 (1.2602)  loss_ce_0: 1.2316 (1.2771)  loss_ce_1: 1.2359 (1.2990)  loss_ce_2: 1.2463 (1.2914)  loss_ce_3: 1.2308 (1.2791)  loss_ce_4: 1.2159 (1.2635)  loss_giou: 0.9565 (0.9928)  loss_giou_0: 1.0021 (1.0370)  loss_giou_1: 0.9705 (1.0099)  loss_giou_2: 0.9611 (1.0012)  loss_giou_3: 0.9504 (0.9969)  loss_giou_4: 0.9535 (0.9948)  cardinality_error_unscaled: 91.9062 (92.1605)  cardinality_error_0_unscaled: 90.8125 (91.2292)  cardinality_error_1_unscaled: 91.6250 (91.9656)  cardinality_error_2_unscaled: 91.9062 (92.1512)  cardinality_error_3_unscaled: 91.7188 (92.0406)  cardinality_error_4_unscaled: 92.0312 (92.2675)  class_error_unscaled: 41.1441 (43.3455)  loss_bbox_unscaled: 0.1111 (0.1061)  loss_bbox_0_unscaled: 0.1146 (0.1125)  loss_bbox_1_unscaled: 0.1102 (0.1093)  loss_bbox_2_unscaled: 0.1092 (0.1077)  loss_bbox_3_unscaled: 0.1102 (0.1068)  loss_bbox_4_unscaled: 0.1093 (0.1065)  loss_ce_unscaled: 0.6017 (0.6301)  loss_ce_0_unscaled: 0.6158 (0.6385)  loss_ce_1_unscaled: 0.6179 (0.6495)  loss_ce_2_unscaled: 0.6231 (0.6457)  loss_ce_3_unscaled: 0.6154 (0.6396)  loss_ce_4_unscaled: 0.6079 (0.6318)  loss_giou_unscaled: 0.4782 (0.4964)  loss_giou_0_unscaled: 0.5010 (0.5185)  loss_giou_1_unscaled: 0.4853 (0.5050)  loss_giou_2_unscaled: 0.4806 (0.5006)  loss_giou_3_unscaled: 0.4752 (0.4984)  loss_giou_4_unscaled: 0.4767 (0.4974)  time: 0.6216  data: 0.0095  max mem: 3135
Test:  [150/157]  eta: 0:00:04  class_error: 56.24  loss: 17.3369 (16.9180)  loss_bbox: 0.5500 (0.5309)  loss_bbox_0: 0.5806 (0.5629)  loss_bbox_1: 0.5648 (0.5469)  loss_bbox_2: 0.5552 (0.5390)  loss_bbox_3: 0.5541 (0.5343)  loss_bbox_4: 0.5535 (0.5329)  loss_ce: 1.2749 (1.2602)  loss_ce_0: 1.2861 (1.2769)  loss_ce_1: 1.3412 (1.2992)  loss_ce_2: 1.3234 (1.2914)  loss_ce_3: 1.3060 (1.2794)  loss_ce_4: 1.2802 (1.2633)  loss_giou: 0.9931 (0.9875)  loss_giou_0: 1.0432 (1.0315)  loss_giou_1: 1.0185 (1.0045)  loss_giou_2: 1.0079 (0.9959)  loss_giou_3: 1.0014 (0.9915)  loss_giou_4: 0.9891 (0.9897)  cardinality_error_unscaled: 91.9062 (92.1509)  cardinality_error_0_unscaled: 90.5312 (91.2096)  cardinality_error_1_unscaled: 91.6250 (91.9466)  cardinality_error_2_unscaled: 91.9688 (92.1401)  cardinality_error_3_unscaled: 91.7188 (92.0284)  cardinality_error_4_unscaled: 92.0312 (92.2535)  class_error_unscaled: 42.6975 (43.5037)  loss_bbox_unscaled: 0.1100 (0.1062)  loss_bbox_0_unscaled: 0.1161 (0.1126)  loss_bbox_1_unscaled: 0.1130 (0.1094)  loss_bbox_2_unscaled: 0.1110 (0.1078)  loss_bbox_3_unscaled: 0.1108 (0.1069)  loss_bbox_4_unscaled: 0.1107 (0.1066)  loss_ce_unscaled: 0.6374 (0.6301)  loss_ce_0_unscaled: 0.6431 (0.6384)  loss_ce_1_unscaled: 0.6706 (0.6496)  loss_ce_2_unscaled: 0.6617 (0.6457)  loss_ce_3_unscaled: 0.6530 (0.6397)  loss_ce_4_unscaled: 0.6401 (0.6316)  loss_giou_unscaled: 0.4966 (0.4938)  loss_giou_0_unscaled: 0.5216 (0.5157)  loss_giou_1_unscaled: 0.5093 (0.5023)  loss_giou_2_unscaled: 0.5040 (0.4979)  loss_giou_3_unscaled: 0.5007 (0.4957)  loss_giou_4_unscaled: 0.4945 (0.4949)  time: 0.6249  data: 0.0100  max mem: 3135
Test:  [156/157]  eta: 0:00:00  class_error: 26.16  loss: 16.7500 (16.9009)  loss_bbox: 0.5306 (0.5326)  loss_bbox_0: 0.5504 (0.5640)  loss_bbox_1: 0.5528 (0.5479)  loss_bbox_2: 0.5475 (0.5401)  loss_bbox_3: 0.5385 (0.5357)  loss_bbox_4: 0.5388 (0.5342)  loss_ce: 1.2544 (1.2579)  loss_ce_0: 1.2722 (1.2752)  loss_ce_1: 1.2799 (1.2968)  loss_ce_2: 1.2662 (1.2890)  loss_ce_3: 1.2477 (1.2769)  loss_ce_4: 1.2631 (1.2612)  loss_giou: 0.9431 (0.9858)  loss_giou_0: 1.0016 (1.0294)  loss_giou_1: 0.9705 (1.0025)  loss_giou_2: 0.9545 (0.9942)  loss_giou_3: 0.9504 (0.9897)  loss_giou_4: 0.9381 (0.9879)  cardinality_error_unscaled: 91.5938 (92.1443)  cardinality_error_0_unscaled: 90.5312 (91.2008)  cardinality_error_1_unscaled: 91.3750 (91.9357)  cardinality_error_2_unscaled: 91.5000 (92.1314)  cardinality_error_3_unscaled: 91.3125 (92.0209)  cardinality_error_4_unscaled: 91.4688 (92.2446)  class_error_unscaled: 41.4397 (43.2709)  loss_bbox_unscaled: 0.1061 (0.1065)  loss_bbox_0_unscaled: 0.1101 (0.1128)  loss_bbox_1_unscaled: 0.1106 (0.1096)  loss_bbox_2_unscaled: 0.1095 (0.1080)  loss_bbox_3_unscaled: 0.1077 (0.1071)  loss_bbox_4_unscaled: 0.1078 (0.1068)  loss_ce_unscaled: 0.6272 (0.6289)  loss_ce_0_unscaled: 0.6361 (0.6376)  loss_ce_1_unscaled: 0.6399 (0.6484)  loss_ce_2_unscaled: 0.6331 (0.6445)  loss_ce_3_unscaled: 0.6239 (0.6385)  loss_ce_4_unscaled: 0.6315 (0.6306)  loss_giou_unscaled: 0.4716 (0.4929)  loss_giou_0_unscaled: 0.5008 (0.5147)  loss_giou_1_unscaled: 0.4853 (0.5012)  loss_giou_2_unscaled: 0.4772 (0.4971)  loss_giou_3_unscaled: 0.4752 (0.4948)  loss_giou_4_unscaled: 0.4691 (0.4939)  time: 0.6040  data: 0.0100  max mem: 3135
Test: Total time: 0:01:40 (0.6410 s / it)
Averaged stats: class_error: 26.16  loss: 16.7500 (16.9009)  loss_bbox: 0.5306 (0.5326)  loss_bbox_0: 0.5504 (0.5640)  loss_bbox_1: 0.5528 (0.5479)  loss_bbox_2: 0.5475 (0.5401)  loss_bbox_3: 0.5385 (0.5357)  loss_bbox_4: 0.5388 (0.5342)  loss_ce: 1.2544 (1.2579)  loss_ce_0: 1.2722 (1.2752)  loss_ce_1: 1.2799 (1.2968)  loss_ce_2: 1.2662 (1.2890)  loss_ce_3: 1.2477 (1.2769)  loss_ce_4: 1.2631 (1.2612)  loss_giou: 0.9431 (0.9858)  loss_giou_0: 1.0016 (1.0294)  loss_giou_1: 0.9705 (1.0025)  loss_giou_2: 0.9545 (0.9942)  loss_giou_3: 0.9504 (0.9897)  loss_giou_4: 0.9381 (0.9879)  cardinality_error_unscaled: 91.5938 (92.1443)  cardinality_error_0_unscaled: 90.5312 (91.2008)  cardinality_error_1_unscaled: 91.3750 (91.9357)  cardinality_error_2_unscaled: 91.5000 (92.1314)  cardinality_error_3_unscaled: 91.3125 (92.0209)  cardinality_error_4_unscaled: 91.4688 (92.2446)  class_error_unscaled: 41.4397 (43.2709)  loss_bbox_unscaled: 0.1061 (0.1065)  loss_bbox_0_unscaled: 0.1101 (0.1128)  loss_bbox_1_unscaled: 0.1106 (0.1096)  loss_bbox_2_unscaled: 0.1095 (0.1080)  loss_bbox_3_unscaled: 0.1077 (0.1071)  loss_bbox_4_unscaled: 0.1078 (0.1068)  loss_ce_unscaled: 0.6272 (0.6289)  loss_ce_0_unscaled: 0.6361 (0.6376)  loss_ce_1_unscaled: 0.6399 (0.6484)  loss_ce_2_unscaled: 0.6331 (0.6445)  loss_ce_3_unscaled: 0.6239 (0.6385)  loss_ce_4_unscaled: 0.6315 (0.6306)  loss_giou_unscaled: 0.4716 (0.4929)  loss_giou_0_unscaled: 0.5008 (0.5147)  loss_giou_1_unscaled: 0.4853 (0.5012)  loss_giou_2_unscaled: 0.4772 (0.4971)  loss_giou_3_unscaled: 0.4752 (0.4948)  loss_giou_4_unscaled: 0.4691 (0.4939)
Accumulating evaluation results...
DONE (t=5.91s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.238
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.077
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.335
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.397
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.137
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.391
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.642
